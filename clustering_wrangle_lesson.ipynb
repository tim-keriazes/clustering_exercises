{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling: Acquisition & Preparation\n",
    "\n",
    "In this lesson, we'll be acquiring and preparing some data from our SQL database.\n",
    "\n",
    "## Learning Goals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "    \n",
    "- Acquire a sample of data from SQL.\n",
    "- Identify null values, which nulls are 'deal-breakers', i.e. rows removed, which nulls should be represented by 0, and which should be replaced by a value from other methods, such as mean.\t\t\n",
    "- Identify outliers and decide what to do with them, if anything (remove, keep as-is, replace).\n",
    "- Data Structure: Aggregate as needed so that every row is an observation and each column is a variable (1 variable and not a measure). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regular imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import env\n",
    "\n",
    "# default pandas decimal number display format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from our acquire.py:\n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "    \n",
    "sql_query = '''\n",
    "SELECT * FROM properties_2017\n",
    "JOIN predictions_2017 USING (parcelid)\n",
    "WHERE transactiondate < '2018'\n",
    "AND propertylandusetypeid = 261;\n",
    "'''\n",
    "    \n",
    "def get_zillow_data():\n",
    "    df = pd.read_sql(sql_query, get_connection('zillow'))\n",
    "    df = df.drop(columns='id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquire our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_zillow_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire & Summarize\n",
    "\n",
    "Let's create a function that gives us a quick summary of our data. What might we want to see?\n",
    "- `.head()`\n",
    "- `.info()`\n",
    "- `.describe()`\n",
    "- `.value_counts()`\n",
    "- null counts (by column and by row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'r0ws: {df.shape[0]}')\n",
    "print(f'co1umns: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls By Column\n",
    "\n",
    "Sure, using built in methods is easy enough. But what about getting our nulls by column and nulls by row? \n",
    "\n",
    "Let's look at nulls by column. Let's start by using `.isnull()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boolean DataFrame can be aggregated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] # Number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isnull().sum()/df.shape[0]*100)[:10] # Percentage of nulls in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_col = pd.DataFrame({'num_rows_missing': df.isnull().sum(), \n",
    "              'percent_rows_missing': (df.isnull().sum() / df.shape[0] * 100)})\n",
    "\n",
    "nulls_col.sort_values(by='num_rows_missing', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put this together in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nulls_by_col(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    rows = df.shape[0]\n",
    "    prcnt_miss = num_missing / rows * 100\n",
    "    cols_missing = pd.DataFrame({'num_rows_missing': num_missing, 'percent_rows_missing': prcnt_miss})\n",
    "    return cols_missing.sort_values(by='num_rows_missing', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls by Row\n",
    "\n",
    "Okay, now how might we look at the number/percent of nulls in each row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the first row has 30 nulls. Lets look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)[df.head(1).isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both `NaN` and `None` are considered as null. Anyways, lets look at our null counts by row again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets turn it into a percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1) / df.shape[1] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_row = pd.DataFrame({'num_cols_missing': df.isnull().sum(axis=1),\n",
    "              'percent_cols_missing': df.isnull().sum(axis=1)/df.shape[1]*100})\n",
    "\n",
    "nulls_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get our `parcelid` back onto this new DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_row = df.merge(nulls_row,\n",
    "                        left_index=True,\n",
    "                        right_index=True)[['parcelid', 'num_cols_missing', 'percent_cols_missing']]\n",
    "\n",
    "nulls_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_row.sort_values(by='num_cols_missing', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nulls_by_row(df):\n",
    "    num_missing = df.isnull().sum(axis=1)\n",
    "    prcnt_miss = num_missing / df.shape[1] * 100\n",
    "    rows_missing = pd.DataFrame({'num_cols_missing': num_missing, 'percent_cols_missing': prcnt_miss})\n",
    "    rows_missing = df.merge(rows_missing,\n",
    "                        left_index=True,\n",
    "                        right_index=True)[['parcelid', 'num_cols_missing', 'percent_cols_missing']]\n",
    "    return rows_missing.sort_values(by='num_cols_with_null', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Combine out Various Techniques into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "    '''\n",
    "    summarize will take in a single argument (a pandas dataframe) \n",
    "    and output to console various statistics on said dataframe, including:\n",
    "    # .head()\n",
    "    # .info()\n",
    "    # .describe()\n",
    "    # .value_counts()\n",
    "    # observation of nulls in the dataframe\n",
    "    '''\n",
    "    print('SUMMARY REPORT')\n",
    "    print('=====================================================\\n\\n')\n",
    "    print('Dataframe head: ')\n",
    "    print(df.head(3))\n",
    "    print('=====================================================\\n\\n')\n",
    "    print('Dataframe info: ')\n",
    "    print(df.info())\n",
    "    print('=====================================================\\n\\n')\n",
    "    print('Dataframe Description: ')\n",
    "    print(df.describe())\n",
    "    num_cols = [col for col in df.columns if df[col].dtype != 'O']\n",
    "    cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "    print('=====================================================')\n",
    "    print('DataFrame value counts: ')\n",
    "    for col in df.columns:\n",
    "        if col in cat_cols:\n",
    "            print(df[col].value_counts(), '\\n')\n",
    "        else:\n",
    "            print(df[col].value_counts(bins=10, sort=False), '\\n')\n",
    "    print('=====================================================')\n",
    "    print('nulls in dataframe by column: ')\n",
    "    print(nulls_by_col(df))\n",
    "    print('=====================================================')\n",
    "    print('nulls in dataframe by row: ')\n",
    "    print(nulls_by_row(df))\n",
    "    print('=====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay we summarized, now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_columns(df, cols_to_remove):\n",
    "    df = df.drop(columns=cols_to_remove)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df, prop_required_columns=0.5, prop_required_row=0.75):\n",
    "    threshold = int(round(prop_required_columns * len(df.index), 0))\n",
    "    df = df.dropna(axis=1, thresh=threshold) #1, or ‘columns’ : Drop columns which contain missing value\n",
    "    threshold = int(round(prop_required_row * len(df.columns), 0))\n",
    "    df = df.dropna(axis=0, thresh=threshold) #0, or ‘index’ : Drop rows which contain missing values.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combining everything in a cleaning function:\n",
    "def data_prep(df, cols_to_remove=[], prop_required_column=0.5, prop_required_row=0.75):\n",
    "    df = remove_columns(df, cols_to_remove)\n",
    "    df = handle_missing_values(df, prop_required_column, prop_required_row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = data_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers\n",
    "- Note your use case\n",
    "- z-score: appropriate for normal data (normally distributed)\n",
    "- Tukey IQR method: not contingent on normality\n",
    "\n",
    "### Tukey Interquartile Range (IQR) Method:\n",
    "- Calculate IQR\n",
    "    - Get Q3 and Q1\n",
    "    - Get difference (q3-q1)\n",
    "    - Establish \"fences\":\n",
    "        - Standard inner fence: k = 1.5\n",
    "        - Standard outer fence: k = 3.0\n",
    "        - Upper bound: q3 + k * IQR\n",
    "        - Lower bound: q1 - k * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_upper_outliers(s, k=1.5):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the upper outliers for the\n",
    "    series.\n",
    "\n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the upper bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + k * iqr\n",
    "    return s.apply(lambda x: max([x - upper_bound, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_upper_outlier_columns(df, k=1.5):\n",
    "    '''\n",
    "    Add a column with the suffix _outliers for all the numeric columns\n",
    "    in the given dataframe.\n",
    "    '''\n",
    "    for col in df.select_dtypes('number'):\n",
    "        df[col + '_outliers_upper'] = get_upper_outliers(df[col], k)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = add_upper_outlier_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outlier_cols = [col for col in df.columns if col.endswith('_outliers_upper')]\n",
    "for col in outlier_cols:\n",
    "    print(col, ': ')\n",
    "    subset = df[col][df[col] > 0]\n",
    "    print(f'Number of Observations Above Upper Bound: {subset.count()}', '\\n')\n",
    "    print(subset.describe())\n",
    "    print('------', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our accomplishments so far\n",
    "We have:\n",
    "- Acquired our data\n",
    "- Examinged the structure and integrity of the data\n",
    "- We have observed univariate descriptive statistics\n",
    "- We have examined null values (in total, by column, and by row)\n",
    "- We have identified upper bound outliers\n",
    "- We have created functions for these processes\n",
    "\n",
    "What we still need to do:\n",
    "- Create functions that identify lower bound outliers\n",
    "- Create functions that act on our outliers (drop them, compress them, etc.)\n",
    "- Create functions/processes that act on nulls beyond simple removal\n",
    "\n",
    "Let's look at applying some of these techniques to a new data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google sheet: https://docs.google.com/spreadsheets/d/14L32EfCmr2asv85i08fla6jf1KakJVcLYaJMkXQ4_p0/edit#gid=0    \n",
    "#Note: Data has been filtered/changed a bit from orginal form to demonstrate null and outlier handling.\n",
    "\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/14L32EfCmr2asv85i08fla6jf1KakJVcLYaJMkXQ4_p0/edit#gid=0'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "\n",
    "df = pd.read_csv(csv_export_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Looks like our `summarize` function uses our `nulls_by_row` function and that was designed for the zillow dataset! I suppose I'll have to tweak that to make it more universal. Let's just move on for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_missing_values(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 8152 entries to 8146 entries. Still a lot of nulls to be managed, but at least we have eliminated the worst offenders by column and by row. \n",
    "\n",
    "The following columns will need to be handled:\n",
    "- `Age`\n",
    "- `Age1stCode`\n",
    "- `Gender`\n",
    "- `YearsCode`\n",
    "- `YearsCodePro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls for YearsCode, YearsCodePro, Age1stCode specifically\n",
    "\n",
    "df = df.dropna(subset=['YearsCode', 'YearsCodePro', 'Age1stCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review values in Gender\n",
    "df.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with mode\n",
    "\n",
    "df.Gender.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df.Gender.fillna(df.Gender.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, but let's not forget what we saw in the value_counts earlier. Several of the columns have weird strings in them. Let's clean these so we can escape this object dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use replace function to reaplce strings with values\n",
    "\n",
    "df.replace('Younger than 5 years',4, inplace = True )\n",
    "df.replace('Older than 85', 85, inplace = True )\n",
    "df.replace('More than 50 years', 50, inplace = True )\n",
    "df.replace('Less than 1 year', 0, inplace = True )\n",
    "\n",
    "# Now we can change datatype for these columns from 'object' to 'int64'\n",
    "\n",
    "df['Age1stCode'] = df.Age1stCode.astype('int64')\n",
    "df['YearsCode'] = df.YearsCode.astype('int64')\n",
    "df['YearsCodePro'] = df.YearsCodePro.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Imputation Technique: KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in train, validate and test dataframes\n",
    "train, test = train_test_split(df,test_size=0.2, random_state=42)\n",
    "train, validate = train_test_split(train,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of three dataframes\n",
    "train.shape,validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KNN imputer to find missing values for 'Age' \n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#Use numeric columns that you want to use for imputation\n",
    "X_numeric = train[['Age', 'Age1stCode', 'YearsCode', 'YearsCodePro']]\n",
    "\n",
    "# define the thing\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# fit the thing (or fit and use with fit_transform) only on train!\n",
    "train_imputed = imputer.fit_transform(X_numeric)\n",
    "train_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there is no null in imputed columns\n",
    "pd.DataFrame(train_imputed).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert imputed array to a dataframe\n",
    "train_imputed = pd.DataFrame(train_imputed, index = train.index)\n",
    "train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign imputed values to the age column\n",
    "train['Age'] = train_imputed[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the transformation on validate and test\n",
    "validate_imputed = imputer.transform(validate[['Age', 'Age1stCode', 'YearsCode', 'YearsCodePro']])\n",
    "test_imputed = imputer.transform(test[['Age', 'Age1stCode', 'YearsCode', 'YearsCodePro']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert arrarys from above cell in dataframes\n",
    "validate_imputed = pd.DataFrame(validate_imputed, index = validate.index)\n",
    "test_imputed = pd.DataFrame(test_imputed, index = test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign imputed to 'Age' column for validate and test dataframes\n",
    "validate['Age'] = validate_imputed[[0]]\n",
    "test['Age'] = test_imputed[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Outlier Technique: Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Age.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q3 = train.Age.quantile([.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "k = 1.5\n",
    "upper_bound_age = q3 + k * iqr\n",
    "upper_bound_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.Age >= upper_bound_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age_capped'] = np.where(train.Age >= upper_bound_age, upper_bound_age, train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Age_capped.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Age', 'Age_capped']].hist(figsize=(12,5), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Within your `codeup-data-science` directory, create a new directory named `clustering-exercises`. This will be where you do your work for this module. Create a repository on GitHub with the same name, and link your local repository to GitHub.\n",
    "\n",
    "Save your clustering work in your `clustering-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "For example, if the\n",
    "exercise directs you to create a file named `myfile.py`, you should have\n",
    "`clustering/myfile.py` in your repository.\n",
    "\n",
    "If a file extension is specified, you should create that specific file. If there\n",
    "is not file extension specified, you may either create a python script or a\n",
    "jupyter notebook for the exercise.\n",
    "\n",
    "!!!tip \"Workflow\"\n",
    "    Throughout the exercises, you may wish to do your work in a notebook, then\n",
    "    transfer any functions you've created to an external python script.\n",
    "\n",
    "    Keep in mind this is not always a linear process! You will probably be\n",
    "    cycling between a notebook and an external python script frequently.\n",
    "\n",
    "    Remember to run your code often to check for correct output and/or errors.\n",
    "\n",
    "## Acquire (acquire.py)\n",
    "\n",
    "### Zillow\n",
    "\n",
    "For the following, iterate through the steps you would take to create functions:\n",
    "Write the code to do the following in a jupyter notebook, test it, convert to functions, then create the file to house those functions. \n",
    "\n",
    "You will have a zillow.ipynb file and a helper file for each section in the pipeline. \n",
    "\n",
    "\n",
    "#### acquire & summarize\n",
    "\n",
    "1. Acquire data from mySQL using the python module to connect and query. You will want to end with a single dataframe. Make sure to include: the logerror, all fields related to the properties that are available. You will end up using all the tables in the database. \n",
    "    - **_Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for `airconditioningtypeid`._**  \n",
    "    - Only include properties with a transaction in 2017, and include only the last transaction for each property (so no duplicate property ID's), along with zestimate error and date of transaction.  \n",
    "    - Only include properties that include a latitude and longitude value.  \n",
    "\n",
    "2. Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)\n",
    "3. Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values. \n",
    "\n",
    "|                          | num_rows_missing | pct_rows_missing  |\n",
    "| ------------------------ |-----------------:| -----------------:|\n",
    "| parcelid                 | 0                | 0.000000          |\n",
    "| airconditioningtypeid    | 29041            | 0.535486          |\n",
    "| architecturalstyletypeid | 54232            | 0.999982          |\n",
    "\n",
    "\n",
    "4. Write a function that takes in a dataframe and returns a dataframe with 3 columns: the number of columns missing, percent of columns missing, and number of rows with n columns missing. Run the function and document takeaways from this on how you want to handle missing values. \n",
    "\n",
    "\n",
    "| num_cols_missing  | pct_cols_missing | num_rows  |\n",
    "| ----------------- |-----------------:| ---------:|\n",
    "| 23                | 38.333           | 108       |\n",
    "| 24                | 40.000           | 123       |\n",
    "| 25                | 41.667           | 5280      |\n",
    "\n",
    "#### Prepare \n",
    "\n",
    "1. Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined.  \n",
    "\n",
    "1. Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).  \n",
    "    - The input:\n",
    "        - A dataframe\n",
    "        - A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column.  i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "        - A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing). \n",
    "    - The output:\n",
    "        - The dataframe with the columns and rows dropped as indicated. *Be sure to drop the columns prior to the rows in your function.*\n",
    "    - hint:\n",
    "        - Look up the dropna documentation. \n",
    "        - You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "        - Make use of inplace, i.e. inplace=True/False.\n",
    "\n",
    "1. Decide how to handle the remaining missing values: \n",
    "    - Fill with constant value.\n",
    "    - Impute with mean, median, mode.\n",
    "    - Drop row/column\n",
    "    \n",
    "\n",
    "#### wrangle_zillow.py\n",
    "\n",
    "Functions of the work above needed to acquire and prepare a new sample of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mall Customers\n",
    "\n",
    "#### notebook\n",
    "\n",
    "1. Acquire data from mall_customers.customers in mysql database. \n",
    "2. Summarize data (include distributions and descriptive statistics).\n",
    "2. Detect outliers using IQR.\n",
    "3. Split data (train, validate, and test split).\n",
    "3. Encode categorical columns using a one hot encoder (pd.get_dummies).\n",
    "4. Handles missing values.\n",
    "5. Scaling\n",
    "\n",
    "#### wrangle_mall.py \n",
    "\n",
    "1. Acquire data from mall_customers.customers in mysql database. \n",
    "2. Split the data into train, validate, and split\n",
    "3. One-hot-encoding (pd.get_dummies)\n",
    "3. Missing values\n",
    "4. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
